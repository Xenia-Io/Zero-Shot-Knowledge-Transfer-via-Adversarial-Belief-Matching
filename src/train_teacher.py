import logging
import sys
import os
import numpy as np
from keras.callbacks import ModelCheckpoint
import argparse
from utils.helpers import *
from models.wide_resnet import *
from data_loader import *

# Prevent reaching to maximum recursion depth in `theano.tensor.grad`
sys.setrecursionlimit(2 ** 20)
logging.basicConfig(level=logging.DEBUG)
np.random.seed(2 ** 10)


def main():
    args = parser.parse_args()

    if args.existing_dataset:
        args.dataset_path = os.path.join(args.dataset_path, args.dataset)


    train_batches, test_batches, len_train_batch = load_dataset(args.batch_size, args.shuffle,
                                                                                args.existing_dataset,
                                                                                dataset=args.dataset,
                                                                                dataset_path=args.dataset_path)
    # Create the required folder structure
    nb_classes = len(test_batches[0][1][0])
    MODEL_PATH = os.environ.get('MODEL_PATH', args.path_to_save_model)
    mk_dir(args.path_to_save_checkpoint)
    CHECKPOINT_PATH = os.environ.get('CHECKPOINT_PATH', os.path.join(args.path_to_save_checkpoint, 'WRN-{0}-{1}'.format(args.model_depth, args.model_width)))
    mk_dir(CHECKPOINT_PATH)
    mk_dir(MODEL_PATH)

    if os.path.exists(args.saved_model):
        model = load_model(args.saved_model)
    else:
        model = WideResNet(args.kernel_init, args.gamma_init, args.dropout, args.learning_rate, args.weight_decay,
                           args.momentum)
        model = model.build_wide_resnet(args.input_shape, nb_classes=nb_classes, d=args.model_depth, k=args.model_width)
        model.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy'])



    model_checkpoint = ModelCheckpoint(CHECKPOINT_PATH + 'weights.{epoch:02d}.h5',
                                       monitor='val_loss',
                                       verbose=1,
                                       save_best_only=True,
                                       save_weights_only=True,
                                       mode='auto')

    logging.debug("Starting to train the model...")
    # fit the model on the batches generated by train_datagen.flow()
    model.fit_generator(train_batches,
                        steps_per_epoch=len_train_batch,
                        epochs=args.epochs,
                        callbacks=[model_checkpoint],
                        validation_data=test_batches[0])

    if args.save_model_weights:
        logging.debug("Saving the model...")
        WEIGHT_PATH = os.path.join(MODEL_PATH, 'weights')
        SUMMARY_PATH = os.path.join(MODEL_PATH, 'model_summary')
        mk_dir(WEIGHT_PATH)
        mk_dir(SUMMARY_PATH)

        with open(os.path.join(SUMMARY_PATH, 'WRN-{0}-{1}-{2}.json'.format(args.model_depth, args.model_width, args.epochs)), 'w') as f:
            f.write(model.to_json())
        model.save(os.path.join(WEIGHT_PATH, 'WRN-{0}-{1}-{2}.h5'.format(args.model_depth, args.model_width, args.epochs)),
                   overwrite=True)

    scores = model.evaluate(test_batches[0][0], test_batches[0][1], len(test_batches[0][0]))
    print('Test loss : %0.5f' % (scores[0]))
    print('Test accuracy = %0.5f' % (scores[1]))


if __name__ == '__main__':
    import os

    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('--existing_dataset', default=False,
                        help='Bool value indicating if the dataset is inbuilt or not, False to use inbuilt dataset')
    parser.add_argument('--dataset_path', default=None, help='downloaded path of dataset to be used if ')
    parser.add_argument('--dataset', default='cifar10', help='cifar10/mnist')
    parser.add_argument('--model_depth', default=40, help='Depth of the WResNet model')
    parser.add_argument('--model_width', default=2, help='Width of the WResNet ')
    parser.add_argument('--dropout', default=0.0, help='dropout probability')
    parser.add_argument('--weight_decay', default=0.0005, help='weight decay while training')
    parser.add_argument('--momentum', default=0.1, help='momentum')
    parser.add_argument('--batch_size', default=64, help='size of the batch of images for training')
    parser.add_argument('--learning_rate', default=1e-5)
    parser.add_argument('--epochs', default=1)
    parser.add_argument('--shuffle', default=True, help='True to Shuffle data')
    parser.add_argument('--save_model_weights', default=True)
    parser.add_argument('--path_to_save_model',
                        default="trained_teacher_weights/")
    parser.add_argument('--path_to_save_checkpoint',
                        default="trained_teacher_checkpoint/")
    parser.add_argument('--kernel_init', default='he_normal', help='Kernel Initializer')
    parser.add_argument('--gamma_init', default='uniform', help='Gamma Initializer')
    parser.add_argument('--input_shape', default=(32, 32, 3), help='Shape of the input for the WResNet')
    parser.add_argument('--saved_model', default='trained_teacher_weights/weights/WRN-40-2-75', help='Prerun model for a few epochs, to continue further training if required')

    main()
